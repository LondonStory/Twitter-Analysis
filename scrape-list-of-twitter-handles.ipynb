{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installs and imports"
      ],
      "metadata": {
        "id": "mniozxm-imoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "#!python3 -m pip install twint\n",
        "!pip install twint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKvYp42rihlp",
        "outputId": "84fb420a-0e05-47e9-b028-05eaadf5e411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: twint in ./src/twint (2.1.21)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from twint) (3.8.3)\n",
            "Requirement already satisfied: aiodns in /usr/local/lib/python3.8/dist-packages (from twint) (3.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from twint) (4.6.3)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.8/dist-packages (from twint) (2.1.7)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.8/dist-packages (from twint) (0.6)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.8/dist-packages (from twint) (8.6.1)\n",
            "Requirement already satisfied: pysocks in /usr/local/lib/python3.8/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from twint) (1.3.5)\n",
            "Requirement already satisfied: aiohttp_socks in /usr/local/lib/python3.8/dist-packages (from twint) (0.7.1)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.8/dist-packages (from twint) (1.1.0)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.8/dist-packages (from twint) (1.17.0)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.8/dist-packages (from twint) (1.1.1)\n",
            "Requirement already satisfied: googletransx in /usr/local/lib/python3.8/dist-packages (from twint) (2.4.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from aiodns->twint) (4.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->twint) (1.3.3)\n",
            "Requirement already satisfied: python-socks[asyncio]<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp_socks->twint) (2.1.1)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.8/dist-packages (from elasticsearch->twint) (8.4.0)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.8/dist-packages (from fake-useragent->twint) (5.10.2)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.8/dist-packages (from geopy->twint) (1.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from googletransx->twint) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->twint) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas->twint) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->twint) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<2,>=1.26.2 in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch->twint) (1.26.14)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch->twint) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=5.0->fake-useragent->twint) (3.12.1)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.15.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->googletransx->twint) (4.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgx-KlkOpKdy",
        "outputId": "9d611b74-9f4b-4b61-fcdb-02d1a8f0c9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.8/dist-packages (1.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "BxYTEeDRjrwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIrnKE-giMer"
      },
      "outputs": [],
      "source": [
        "import twint\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset and extract usernames"
      ],
      "metadata": {
        "id": "LVJ0E836kZ57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZoagYnJnj_l",
        "outputId": "be557751-7861-4229-fe67-d4ee55d14287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CSV"
      ],
      "metadata": {
        "id": "p10ZvTJMlqPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv ('/content/drive/MyDrive/replies-larrouturou-annotated-botometer.csv')\n",
        "df = pd.read_csv ('/content/drive/MyDrive/replies-alviinaalametsa-annotated-botometer.csv')"
      ],
      "metadata": {
        "id": "v_WBRNNilRh9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd-MR44LltCy",
        "outputId": "c79b98df-26a8-4fa7-92a7-25198d598ce0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place',\n",
              "       'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str',\n",
              "       'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
              "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
              "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
              "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
              "       'trans_dest', 'sentiment', 'annotator', 'annotation_id', 'updated_at',\n",
              "       'lead_time', 'botometerCapEng.score', 'botometerCapUni.score',\n",
              "       'botometerOverallRawEng.score', 'botometerOverallRawUni.score',\n",
              "       'botometerDisplayEng.score', 'botometerDisplayUni.score'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract usernames of potentially bot accounts"
      ],
      "metadata": {
        "id": "wc4ego_ol3hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bot = df[df['botometerOverallRawUni.score'] > 0.6]"
      ],
      "metadata": {
        "id": "Wy8IE9_Sl7D9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicates"
      ],
      "metadata": {
        "id": "mh9DBCVQnXOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bot_unique = df_bot.drop_duplicates(subset=['username'])"
      ],
      "metadata": {
        "id": "l8wo17BQnPIy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to a list"
      ],
      "metadata": {
        "id": "nqcLmy8MoGM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username_list_bot = df_bot_unique ['username'].tolist()\n",
        "print(len(username_list_bot))\n",
        "print (username_list_bot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anVf0YC-l1qa",
        "outputId": "35612f97-284e-4513-83cc-6a2e4a4bb7de"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "['vt82878', 'sanjuktaroyvai1', 'ambar_dave', 'nameshakehe', 'SanatanForce', 'vedalasrinivas2', 'kirudevadiga', 'PunctureWalIa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract usernames of 100% authentic/ real accounts"
      ],
      "metadata": {
        "id": "CnXuy2gtntdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_real = df[df['botometerOverallRawUni.score'] < 0.1]"
      ],
      "metadata": {
        "id": "YQQN1TnWn0sL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicates"
      ],
      "metadata": {
        "id": "WoWAWg2_oDHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_real_unique = df_real.drop_duplicates(subset=['username'])"
      ],
      "metadata": {
        "id": "8jqUv8Ven9H8"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to a list"
      ],
      "metadata": {
        "id": "5jBXcV2bx47u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username_list_real = df_real_unique ['username'].tolist()\n",
        "print(len(username_list_real))\n",
        "print (username_list_real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N6GVwQVoJdF",
        "outputId": "a25a73a8-7840-4d19-b9c3-fae713437406"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n",
            "['tarunsharma005', 'ShekharrG', 'Sunishen', 'khab33b', 'IndiaFirst2022', 'bhanuvarma007', 'Ksharkhowa', 'tarunlalan', 'roshovani', 'pairamblr', 'tellitaly', 'spandakarika108', 'MohanKumarL11', 'Nishwins1', 'abhiawakes', 'niranjan_shaha', 'dcemeterygirl', 'UnApologeticM1', 'citizenNA', 'Prasenjit97m', 'AkashD60789383', 'Arjun_S_R', 'dschamyal', 'ScorpionHere', 'PrasadSatya10', 'VickyHanumant', 'woke_less', 'ib4uanytime', 'SachinDPatange', 'DiVpops', 'MiFe007', 'bhattketan1468', '_hamza_iftikhar', 'Hilale_pakistan', 'DrBharatbhushan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data acquisition function"
      ],
      "metadata": {
        "id": "bf2MKkLgir2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out [Twint's Github page](https://github.com/twintproject/twint) for more details."
      ],
      "metadata": {
        "id": "SS3AyM8HtFtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_acq (user_name, num, since_date, until_date):\n",
        "    print (\"======================================\")\n",
        "    print(\":: Acquiring Data for\", user_name, \"::\")\n",
        "    print (\"======================================\")\n",
        "\n",
        "    # Configure\n",
        "    c = twint.Config()\n",
        "\n",
        "    #c.Search = search_string    # uncomment this if you want to scrape using a search_string, and provide this as the input of this function\n",
        "    c.Username = user_name\n",
        "\n",
        "    c.Since = since_date\n",
        "    c.Until = until_date\n",
        "\n",
        "    #c.Limit = num\n",
        "\n",
        "    #c.Lang = 'nl'\n",
        "    #c.Translate = True \n",
        "    #c.TranslateDest = \"en\"\n",
        "\n",
        "    c.Store_object =  True\n",
        "    c.User_full = True\n",
        "    c.Profile_full = True\n",
        "    c.Hide_output = True\n",
        "\n",
        "    c.Pandas = True\n",
        "    twint.run.Search(c)\n",
        "\n",
        "    return twint.storage.panda.Tweets_df"
      ],
      "metadata": {
        "id": "xRpcri8lirfT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters to acquire tweets"
      ],
      "metadata": {
        "id": "pki5EVnEmFII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the list of usernames\n",
        "\n",
        "#----larrouturou-----\n",
        "#username_list = username_list_bot\n",
        "#username_list = list (set(username_list_real) - set(['mrImteyaz8','TheWhiteWaIker','frankblunt2021', 'maverick9762', 'dinakar24', 'Prasenjit97m', 'omprakash2711']))\n",
        "\n",
        "\n",
        "#----alviinaalametsa----\n",
        "username_list = username_list_bot\n",
        "#username_list = list (set(username_list_real) - set(['spandakarika108', 'Prasenjit97m']))\n",
        "\n",
        "\n",
        "# provide the number of most recent tweets you want to scrape\n",
        "N = 10000\n",
        "\n",
        "# provide the beginning date ('YYYY-MM-DD' format)\n",
        "since_date = '2022-01-01 12:00:00'\n",
        "\n",
        "# provide the end date\n",
        "until_date = '2022-12-31 12:00:00'"
      ],
      "metadata": {
        "id": "8HHhO_demHRm"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the scrapper function to extract the historical dataset and save"
      ],
      "metadata": {
        "id": "9cBsvnYum9ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(username_list)):\n",
        "  # run the scrapper function\n",
        "  feed_df = data_acq (str(username_list[i]), N, since_date, until_date)\n",
        "\n",
        "  # convert the output into a dataframe\n",
        "  df = pd.DataFrame (feed_df)\n",
        "\n",
        "  # print the head of the dataframe\n",
        "  print (df.head())\n",
        "\n",
        "  # save the dataframe as csv to your Google Drive -- \n",
        "  #================= CHANGE THE PATH BELOW ACCORDING TO THE SEARCH ABOVE==================\n",
        "  df.to_csv ('/content/drive/My Drive/Twitter-Accounts-Historical-Data/Alviinaalametsa/Bot/%s.csv' %username_list[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu0hXXcnm6Gm",
        "outputId": "5bb0805c-2922-4def-d1a7-b782fc61f02b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================\n",
            ":: Acquiring Data for vt82878 ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "======================================\n",
            ":: Acquiring Data for sanjuktaroyvai1 ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "======================================\n",
            ":: Acquiring Data for ambar_dave ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "                    id      conversation_id    created_at  \\\n",
            "0  1609017377185992705  1608974071399878656  1.672455e+12   \n",
            "1  1608852017921036288  1608794081156804608  1.672415e+12   \n",
            "2  1608849429653778432  1608831426698579973  1.672415e+12   \n",
            "3  1608589925355782145  1608589925355782145  1.672353e+12   \n",
            "4  1607937762484948993  1607778873961500672  1.672197e+12   \n",
            "\n",
            "                  date timezone place  \\\n",
            "0  2022-12-31 02:43:31    +0000         \n",
            "1  2022-12-30 15:46:26    +0000         \n",
            "2  2022-12-30 15:36:09    +0000         \n",
            "3  2022-12-29 22:24:59    +0000         \n",
            "4  2022-12-28 03:13:31    +0000         \n",
            "\n",
            "                                               tweet language hashtags  \\\n",
            "0                              @CollinRugg Hunter's?       en       []   \n",
            "1  @jozefdemestr @MacaesBruno Singapore also supp...       en       []   \n",
            "2                                    @MacaesBruno ☺️      und       []   \n",
            "3  @vtchakarova @tanvi_madan  ☺️   https://t.co/N...      und       []   \n",
            "4    @Newsumindia @dpradhanbjp Which Vikram Samvat?😢       cs       []   \n",
            "\n",
            "  cashtags  ...  geo source user_rt_id user_rt  retweet_id  \\\n",
            "0       []  ...                                              \n",
            "1       []  ...                                              \n",
            "2       []  ...                                              \n",
            "3       []  ...                                              \n",
            "4       []  ...                                              \n",
            "\n",
            "                                            reply_to retweet_date translate  \\\n",
            "0  [{'screen_name': 'CollinRugg', 'name': 'Collin...                          \n",
            "1  [{'screen_name': 'jozefdemestr', 'name': 'Joze...                          \n",
            "2  [{'screen_name': 'MacaesBruno', 'name': 'Bruno...                          \n",
            "3                                                 []                          \n",
            "4  [{'screen_name': 'Newsumindia', 'name': 'Newsu...                          \n",
            "\n",
            "  trans_src  trans_dest  \n",
            "0                        \n",
            "1                        \n",
            "2                        \n",
            "3                        \n",
            "4                        \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "======================================\n",
            ":: Acquiring Data for nameshakehe ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "                    id      conversation_id    created_at  \\\n",
            "0  1609039302159392768  1608987609262600193  1.672460e+12   \n",
            "1  1608720776429174786  1608154552653561866  1.672384e+12   \n",
            "2  1608720435172237313  1608154552653561866  1.672384e+12   \n",
            "3  1608719436592349184  1608154552653561866  1.672384e+12   \n",
            "4  1608716078443810816  1608285302577324032  1.672383e+12   \n",
            "\n",
            "                  date timezone place  \\\n",
            "0  2022-12-31 04:10:38    +0000         \n",
            "1  2022-12-30 07:04:56    +0000         \n",
            "2  2022-12-30 07:03:34    +0000         \n",
            "3  2022-12-30 06:59:36    +0000         \n",
            "4  2022-12-30 06:46:16    +0000         \n",
            "\n",
            "                                               tweet language hashtags  \\\n",
            "0                              @Nothennyfr Skip JEE.       en       []   \n",
            "1             @SerhiiPylypenk2 @MacaesBruno can say.       en       []   \n",
            "2  @SerhiiPylypenk2 @MacaesBruno Whether we suppo...       en       []   \n",
            "3  @SerhiiPylypenk2 @MacaesBruno When Chinese wou...       en       []   \n",
            "4  @ManchinEnjoyer @mattyglesias Hardly anything ...       en       []   \n",
            "\n",
            "  cashtags  ...  geo source user_rt_id user_rt  retweet_id  \\\n",
            "0       []  ...                                              \n",
            "1       []  ...                                              \n",
            "2       []  ...                                              \n",
            "3       []  ...                                              \n",
            "4       []  ...                                              \n",
            "\n",
            "                                            reply_to retweet_date translate  \\\n",
            "0  [{'screen_name': 'Nothennyfr', 'name': 'father...                          \n",
            "1  [{'screen_name': 'SerhiiPylypenk2', 'name': 'S...                          \n",
            "2  [{'screen_name': 'SerhiiPylypenk2', 'name': 'S...                          \n",
            "3  [{'screen_name': 'SerhiiPylypenk2', 'name': 'S...                          \n",
            "4  [{'screen_name': 'ManchinEnjoyer', 'name': 'Ba...                          \n",
            "\n",
            "  trans_src  trans_dest  \n",
            "0                        \n",
            "1                        \n",
            "2                        \n",
            "3                        \n",
            "4                        \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "======================================\n",
            ":: Acquiring Data for SanatanForce ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "                    id      conversation_id    created_at  \\\n",
            "0  1604065507615084544  1604063930267336704  1.671274e+12   \n",
            "1  1589119475479384066  1589115092859486208  1.667711e+12   \n",
            "2  1588894725570187265  1588888257085206529  1.667657e+12   \n",
            "3  1588782112630779905  1588768946970570752  1.667630e+12   \n",
            "4  1581944962761228288  1581894733873713152  1.666000e+12   \n",
            "\n",
            "                  date timezone place  \\\n",
            "0  2022-12-17 10:46:33    +0000         \n",
            "1  2022-11-06 04:56:22    +0000         \n",
            "2  2022-11-05 14:03:17    +0000         \n",
            "3  2022-11-05 06:35:48    +0000         \n",
            "4  2022-10-17 09:47:24    +0000         \n",
            "\n",
            "                                               tweet language hashtags  \\\n",
            "0  @theliverdr Sir, just a query.. Is Lipaglyn go...       en       []   \n",
            "1  @ragiing_bull I recommend two places for authe...       en       []   \n",
            "2  @UnSubtleDesi Any update on Tommy robinson int...       en       []   \n",
            "3  @arifayyub @elonmusk Nobody forced you to live...       en       []   \n",
            "4  @KhaledBeydoun Yes.. we're feeding Umar Khalid...       en       []   \n",
            "\n",
            "  cashtags  ...  geo source user_rt_id user_rt  retweet_id  \\\n",
            "0       []  ...                                              \n",
            "1       []  ...                                              \n",
            "2       []  ...                                              \n",
            "3       []  ...                                              \n",
            "4       []  ...                                              \n",
            "\n",
            "                                            reply_to retweet_date translate  \\\n",
            "0  [{'screen_name': 'theliverdr', 'name': 'TheLiv...                          \n",
            "1  [{'screen_name': 'ragiing_bull', 'name': 'Deep...                          \n",
            "2  [{'screen_name': 'UnSubtleDesi', 'name': 'Nupu...                          \n",
            "3  [{'screen_name': 'arifayyub', 'name': 'Arif Ay...                          \n",
            "4  [{'screen_name': 'KhaledBeydoun', 'name': 'Kha...                          \n",
            "\n",
            "  trans_src  trans_dest  \n",
            "0                        \n",
            "1                        \n",
            "2                        \n",
            "3                        \n",
            "4                        \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "======================================\n",
            ":: Acquiring Data for vedalasrinivas2 ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "======================================\n",
            ":: Acquiring Data for kirudevadiga ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "======================================\n",
            ":: Acquiring Data for PunctureWalIa ::\n",
            "======================================\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    }
  ]
}